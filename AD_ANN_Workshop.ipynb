{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2hP3w4v5sE3kL5zqK7p8F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/AD_ANN_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data-Driven Modeling of Anaerobic Digesters Using Artificial Neural Networks (ANNs)\n",
        "\n",
        "![Anaerobic Digester](https://via.placeholder.com/800x200?text=Anaerobic+Digester+Illustration)  <!-- Optional: Replace with a real image URL if desired -->\n",
        "\n",
        "## Workshop Overview\n",
        "This notebook provides a beginner-friendly, hands-on workshop on using Artificial Neural Networks (ANNs) for modeling anaerobic digesters (AD) to predict biogas production. It is targeted at biogas researchers, AD plant operators, consultants, and sustainability professionals with little to no programming experience.\n",
        "\n",
        "### Key Features:\n",
        "- **No Installation Required**: Runs entirely in Google Colab.\n",
        "- **Interactive**: Train a model, evaluate it, and play with predictions using your own inputs.\n",
        "- **Well-Documented**: Each step includes explanations, code comments, and tips.\n",
        "- **Data**: Uses synthetic AD data (`AD_Synthetic_Data.xlsx`). Upload your own or use the provided one.\n",
        "- **Topics Covered**: Data loading, feature selection, preprocessing, ANN building/training, evaluation, visualization, and interactive prediction.\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic understanding of anaerobic digestion (AD) processes.\n",
        "- Access to Google Colab (free, browser-based).\n",
        "- The dataset file: `AD_Synthetic_Data.xlsx` (upload it in Step 1).\n",
        "- Optional: Background on ANNs (simple feedforward networks).\n",
        "\n",
        "### How to Use This Notebook:\n",
        "1. Open in Colab (click the badge above).\n",
        "2. Run cells sequentially (Shift+Enter or click the play button).\n",
        "3. Upload your data file when prompted.\n",
        "4. Experiment by modifying parameters (e.g., number of epochs, top features).\n",
        "5. At the end, play with the model by entering custom inputs.\n",
        "\n",
        "If you encounter issues, check the output logs or restart the runtime (Runtime > Restart runtime). For real-world use, validate with actual plant data!"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-1-upload-the-data-file\"></a>\n",
        "## Step 1: Upload the Data File\n",
        "\n",
        "**Description**: Upload the synthetic dataset `AD_Synthetic_Data.xlsx`. This file contains time-series data on AD inputs (e.g., substrate concentrations) and output (biogas flow `q_gas`).\n",
        "\n",
        "**Why?** Colab runs in the cloud, so we need to upload local files.\n",
        "\n",
        "**Tips**: \n",
        "- If you don't have the file, generate synthetic data or use a placeholder.\n",
        "- After uploading, the file is available in the session storage."
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Dynamically get the uploaded file name (assumes one file)\n",
        "file_path = list(uploaded.keys())[0]  # Adjust if multiple files\n",
        "print(f\"Uploaded file: {file_path}\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-2-import-libraries\"></a>\n",
        "## Step 2: Import Libraries\n",
        "\n",
        "**Description**: Load necessary Python libraries for data handling, modeling, and visualization.\n",
        "\n",
        "**Libraries Explained**:\n",
        "- `numpy`, `pandas`: Data manipulation.\n",
        "- `matplotlib`: Plotting.\n",
        "- `sklearn`: Preprocessing, metrics, feature selection.\n",
        "- `tensorflow.keras`: Building and training ANNs.\n",
        "- `time`: Measure execution time.\n",
        "\n",
        "**Note**: Colab has these pre-installed; no need to install anything."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-3-load-the-dataset\"></a>\n",
        "## Step 3: Load the Dataset\n",
        "\n",
        "**Description**: Read the Excel file into a Pandas DataFrame and preview the first few rows.\n",
        "\n",
        "**Why?** To inspect the data structure (columns, values).\n",
        "\n",
        "**Tips**: Ensure the sheet name is 'Sheet1'. If your data is CSV, use `pd.read_csv()` instead."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "print(\"Data preview (first 5 rows):\")\n",
        "display(df.head())  # Nicer table output in Colab"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-4-define-inputs-and-output\"></a>\n",
        "## Step 4: Define Inputs and Output\n",
        "\n",
        "**Description**: Specify input features (predictors) and the target output (`q_gas`).\n",
        "\n",
        "**Features Explained** (based on typical AD models like ADM1):\n",
        "- `S_*`: Soluble substrates (e.g., sugars `S_su`, amino acids `S_aa`).\n",
        "- `X_*`: Particulate biomass and composites.\n",
        "- `Q`: Flow rate, `T (C)`: Temperature, etc.\n",
        "\n",
        "**Why?** ANNs need clear inputs/outputs to learn mappings."
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['S_su', 'S_aa', 'S_fa', 'S_va', 'S_bu', 'S_pro', 'S_ac', 'S_IC', 'S_IN', \n",
        "            'S_I', 'Q', 'T (C)', 'X_xc', 'X_ch', 'X_pr', 'X_li', 'X_su', 'X_aa', \n",
        "            'X_fa', 'X_c4', 'X_pro', 'X_ac', 'X_h2', 'X_I', 'S_cation', 'S_anion']\n",
        "\n",
        "label = 'q_gas'\n",
        "\n",
        "X = df[features]\n",
        "y = df[label]"
      ],
      "metadata": {
        "id": "define-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-5-feature-selection-using-mutual-information\"></a>\n",
        "## Step 5: Feature Selection Using Mutual Information\n",
        "\n",
        "**Description**: Rank features by their mutual information with the target. Select top 10.\n",
        "\n",
        "**Why?** Reduces complexity, prevents overfitting, focuses on influential variables (e.g., temperature might be key).\n",
        "\n",
        "**Tips**: Adjust `top_n` to experiment. Higher MI score = stronger relationship."
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
        "mi_df = pd.DataFrame({'Feature': features, 'Mutual_Information': mi_scores})\n",
        "mi_df.sort_values(by='Mutual_Information', ascending=False, inplace=True)\n",
        "print(\"Feature importance based on Mutual Information:\")\n",
        "display(mi_df)\n",
        "\n",
        "# Select top N features\n",
        "top_n = 10  # You can change this (e.g., to 5 or 15)\n",
        "top_features = mi_df['Feature'].head(top_n).tolist()\n",
        "print(f\"Top {top_n} features selected for training: {top_features}\")\n",
        "\n",
        "X_top = df[top_features]"
      ],
      "metadata": {
        "id": "feature-selection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-6-data-preprocessing\"></a>\n",
        "## Step 6: Data Preprocessing\n",
        "\n",
        "**Description**: Scale features and target to [0,1] range; split into train/test sets.\n",
        "\n",
        "**Why?** ANNs perform better with normalized data. Train/test split evaluates generalization.\n",
        "\n",
        "**Tips**: No shuffling for time-series data to preserve order. Test size=0.2 means 20% for testing."
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_top)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data (80% train, 20% test, no shuffle for time series)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-7-build-the-ann-model\"></a>\n",
        "## Step 7: Build the ANN Model\n",
        "\n",
        "**Description**: Create a sequential ANN with input, hidden, and output layers.\n",
        "\n",
        "**Architecture**:\n",
        "- Input: Matches number of top features.\n",
        "- Hidden: 64 and 32 neurons with ReLU activation (introduces non-linearity).\n",
        "- Output: 1 neuron for regression (predicting continuous `q_gas`).\n",
        "\n",
        "**Why?** Simple feedforward network suitable for tabular data prediction.\n",
        "\n",
        "**Tips**: Experiment with more layers or neurons (e.g., add `model.add(Dense(16, activation='relu'))`)."
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
        "model.add(Dense(32, activation='relu'))  # Second hidden layer\n",
        "model.add(Dense(1))  # Output layer\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "build-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-8-train-the-model\"></a>\n",
        "## Step 8: Train the Model\n",
        "\n",
        "**Description**: Fit the model to training data, validate on test data.\n",
        "\n",
        "**Parameters**:\n",
        "- `epochs=50`: Number of training iterations.\n",
        "- `batch_size=32`: Data chunks per update.\n",
        "\n",
        "**Why?** Training minimizes loss (error).\n",
        "\n",
        "**Tips**: If loss doesn't decrease, increase epochs or adjust architecture. Monitor for overfitting (train loss << val loss)."
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
        "                    validation_data=(X_test, y_test), verbose=1)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-9-make-predictions-and-evaluate\"></a>\n",
        "## Step 9: Make Predictions and Evaluate\n",
        "\n",
        "**Description**: Predict on train/test sets, inverse scale, and compute metrics.\n",
        "\n",
        "**Metrics**:\n",
        "- R²: Goodness of fit (1 = perfect).\n",
        "- RMSE: Root mean squared error (lower = better).\n",
        "\n",
        "**Why?** Quantifies model accuracy.\n",
        "\n",
        "**Tips**: Aim for test R² > 0.8 for good performance. If low, retrain with more data/features."
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse scaling to original units\n",
        "y_train_orig = scaler_y.inverse_transform(y_train)\n",
        "y_train_pred_orig = scaler_y.inverse_transform(y_train_pred)\n",
        "y_test_orig = scaler_y.inverse_transform(y_test)\n",
        "y_test_pred_orig = scaler_y.inverse_transform(y_test_pred)\n",
        "\n",
        "# Metrics\n",
        "r2_train = r2_score(y_train_orig, y_train_pred_orig)\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_train_pred_orig))\n",
        "r2_test = r2_score(y_test_orig, y_test_pred_orig)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_orig))\n",
        "\n",
        "print(f'Training R²: {r2_train:.4f} (how well it fits train data, 1=perfect), RMSE: {rmse_train:.4f} (average error)')\n",
        "print(f'Test R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}')"
      ],
      "metadata": {
        "id": "predict-eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-10-visualize-results\"></a>\n",
        "## Step 10: Visualize Results\n",
        "\n",
        "**Description**: Plot actual vs. predicted, training loss, and feature importance.\n",
        "\n",
        "**Why?** Visuals help interpret model performance (e.g., does it capture trends?).\n",
        "\n",
        "**Tips**: Look for overlapping lines in predictions. Stable loss = converged model."
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual vs Predicted (Train)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_train_orig, label='Actual Train')\n",
        "plt.plot(y_train_pred_orig, label='Predicted Train')\n",
        "plt.title('Training Set: Actual vs Predicted Biogas Production')\n",
        "plt.ylabel('q_gas')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Actual vs Predicted (Test)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test_orig, label='Actual Test')\n",
        "plt.plot(y_test_pred_orig, label='Predicted Test')\n",
        "plt.title('Test Set: Actual vs Predicted Biogas Production')\n",
        "plt.ylabel('q_gas')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Training Loss\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "top_mi_features = mi_df.head(top_n).sort_values(by='Mutual_Information')\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(top_mi_features['Feature'], top_mi_features['Mutual_Information'], color='skyblue')\n",
        "plt.xlabel('Mutual Information Score')\n",
        "plt.title(f'Top {top_n} Features Based on Mutual Information')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-11-export-results-optional\"></a>\n",
        "## Step 11: Export Results (Optional)\n",
        "\n",
        "**Description**: Save actual vs. predicted values to an Excel file and download it.\n",
        "\n",
        "**Why?** For offline analysis or reporting.\n",
        "\n",
        "**Tips**: File appears in Colab's file browser; auto-downloads via `files.download()`."
      ],
      "metadata": {
        "id": "step11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_output_df = pd.DataFrame({\n",
        "    'Actual_Train': y_train_orig.ravel(),\n",
        "    'Predicted_Train': y_train_pred_orig.ravel()\n",
        "})\n",
        "\n",
        "test_output_df = pd.DataFrame({\n",
        "    'Actual_Test': y_test_orig.ravel(),\n",
        "    'Predicted_Test': y_test_pred_orig.ravel()\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter('predicted_vs_actual_top_features_ANN.xlsx') as writer:\n",
        "    train_output_df.to_excel(writer, sheet_name='Train', index=False)\n",
        "    test_output_df.to_excel(writer, sheet_name='Test', index=False)\n",
        "\n",
        "files.download('predicted_vs_actual_top_features_ANN.xlsx')\n",
        "print('Results exported! Check your downloads.')"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"step-12-play-with-the-model\"></a>\n",
        "## Step 12: Play with the Model!\n",
        "\n",
        "**Description**: Input custom values for top features and predict `q_gas`.\n",
        "\n",
        "**Why?** Simulate scenarios (e.g., \"What if I increase temperature?\").\n",
        "\n",
        "**Tips**: \n",
        "- Use realistic values based on AD knowledge.\n",
        "- Uncomment the input loop for interactive entry.\n",
        "- Re-run after changing inputs to see new predictions."
      ],
      "metadata": {
        "id": "step12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input: Replace with your values (order matches top_features)\n",
        "print(f\"Enter values for: {top_features}\")\n",
        "user_input = np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]])  # Dummy; change this!\n",
        "\n",
        "# For interactive input (uncomment below):\n",
        "# user_values = []\n",
        "# for feature in top_features:\n",
        "#     val = float(input(f\"Enter value for {feature}: \"))\n",
        "#     user_values.append(val)\n",
        "# user_input = np.array([user_values])\n",
        "\n",
        "# Scale and predict\n",
        "user_scaled = scaler_X.transform(user_input)\n",
        "user_pred_scaled = model.predict(user_scaled)\n",
        "user_pred = scaler_y.inverse_transform(user_pred_scaled)\n",
        "\n",
        "print(f\"Predicted q_gas: {user_pred[0][0]:.4f}\")"
      ],
      "metadata": {
        "id": "play-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "You've completed the workshop! You've loaded data, selected features, trained an ANN, evaluated it, and even made custom predictions.\n",
        "\n",
        "**Next Steps**:\n",
        "- Apply to your real AD data.\n",
        "- Experiment: Try LSTM for time-series if data is sequential.\n",
        "- Share on GitHub: Star/fork if useful!\n",
        "\n",
        "Questions? Open an issue on the repo or comment below. Happy biogas modeling! 🌱🔬"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
