{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2hP3w4v5sE3kL5zqK7p8F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rm01243/Data-Driven-Modelling-for-AD-Workshop/blob/main/AD_ANN_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data-Driven Modelling of Anaerobic Digesters Using Artificial Neural Networks (ANNs)\n",
        "\n",
        "![Data Driven Modelling Image](https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/Data%20Driven%20Modelling%20Image.png)\n",
        "\n",
        "## Workshop Overview\n",
        "This notebook provides a beginner-friendly, hands-on workshop on using Artificial Neural Networks (ANNs) for modelling anaerobic digesters (AD) to predict biogas production. It is targeted at biogas researchers, AD plant operators, consultants, and sustainability professionals with little to no programming or modelling experience.\n",
        "\n",
        "### What is Anaerobic Digestion?\n",
        "Anaerobic digestion is a biological process where microorganisms break down organic material in the absence of oxygen, producing biogas (mainly methane) as a renewable energy source. Modelling this process helps predict biogas output based on inputs like temperature and substrate levels, aiding in optimisation without complex physics-based equations.\n",
        "\n",
        "### What are ANNs?\n",
        "Artificial Neural Networks are computational models inspired by the human brain. They learn patterns from data to make predictions. In this workshop, we'll use a simple ANN to predict biogas flow (`q_gas`) from AD parameters.\n",
        "\n",
        "### Key Features:\n",
        "- **No Installation Required**: Runs entirely in Google Colab, a free online tool that executes Python code in your browser.\n",
        "- **Interactive**: Train a model, evaluate it, and see results with plots.\n",
        "- **Data**: Uses synthetic AD data (`AD_Synthetic_Data.xlsx`) loaded directly from the GitHub repository. Synthetic data is artificially generated to mimic real AD processes for demonstration.\n",
        "- **Topics Covered**: Data loading, feature selection, preprocessing, ANN building/training, evaluation, and visualisation.\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic understanding of anaerobic digestion (AD) processes (e.g., what substrates and temperature affect biogas).\n",
        "- Access to Google Colab (free, browser-based—no software needed).\n",
        "- No coding background required—we'll explain every line!\n",
        "\n",
        "### How to Use This Notebook:\n",
        "1. Open in Colab (click the badge above).\n",
        "2. Run cells sequentially (Shift+Enter or click the play button). Markdown cells (like this) explain concepts; code cells execute Python code.\n",
        "3. Experiment by modifying parameters (e.g., number of epochs in training, or top features in selection) and re-run cells to see changes.\n",
        "4. If something goes wrong, restart the runtime (Runtime > Restart runtime) and re-run from the top."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "**Description for Beginners**: Libraries are pre-written code packages that provide tools for tasks like data handling or modelling. We 'import' them to use their functions. For example, Pandas helps read Excel files, and Keras builds the ANN.\n",
        "\n",
        "**Libraries Explained**:\n",
        "- `numpy` (np): For numerical operations, like arrays.\n",
        "- `pandas` (pd): For loading and manipulating data tables.\n",
        "- `matplotlib.pyplot` (plt): For creating plots and visualisations.\n",
        "- `time`: To measure how long code takes to run.\n",
        "- `sklearn` modules: For splitting data, scaling, metrics, and feature selection.\n",
        "- `tensorflow.keras`: For building and training the ANN (TensorFlow is the backend, Keras is the user-friendly interface).\n",
        "\n",
        "**Why Do This?** These tools save time—no need to write everything from scratch.\n",
        "\n",
        "**Note**: Colab has these pre-installed, so just run the cell."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # For numerical computations and arrays\n",
        "import pandas as pd  # For reading and handling data in table format\n",
        "import matplotlib.pyplot as plt  # For creating plots and graphs\n",
        "import time  # To time how long the code takes\n",
        "import tensorflow as tf  # For setting random seed\n",
        "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
        "from sklearn.preprocessing import MinMaxScaler  # To scale data to a 0-1 range\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # To evaluate model performance\n",
        "from sklearn.feature_selection import mutual_info_regression  # For selecting important features\n",
        "from tensorflow.keras.models import Sequential  # To build the ANN layer by layer\n",
        "from tensorflow.keras.layers import Dense  # To add layers to the ANN\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "**Description for Beginners**: We read the Excel file from the GitHub repository into a 'DataFrame' (df), which is like an Excel table in Python. This lets us access columns easily. We'll display the first 5 rows to check the data.\n",
        "\n",
        "**Why Do This?** To ensure the data is loaded correctly and understand its structure.\n",
        "\n",
        "**Tips**: If the sheet name is different, change 'Sheet1'. For your own data, you can replace the URL with your file's raw GitHub URL. Update column names in the data file to full forms like 'Carbohydrates' for clarity, and adjust the 'features' list in Step 3 accordingly."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/AD_Synthetic_Data.xlsx'  # URL to the data file in the repo\n",
        "df = pd.read_excel(data_url, sheet_name='Sheet1')  # Reads the Excel file from URL into a DataFrame\n",
        "print(\"Data preview (first 5 rows):\")  # Prints a message\n",
        "display(df.head())  # Shows the first 5 rows in a nice table format"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Inputs and Output\n",
        "\n",
        "**Description for Beginners**: Inputs (features) are the variables that influence biogas production, like temperature or substrate levels. The output (label) is what we want to predict: 'q_gas' (biogas flow rate). We separate them into X (inputs) and y (output).\n",
        "\n",
        "**Features Explained** (based on typical AD models like ADM1):\n",
        "- `S_*`: Soluble substrates (e.g., `S_su` = sugars, `S_aa` = amino acids)—these are broken down by microbes.\n",
        "- `X_*`: Particulate biomass (e.g., `X_su` = sugar degraders).\n",
        "- `Q`: Inflow rate, `T (C)`: Temperature in Celsius—key for microbial activity.\n",
        "- Others: Ions, inerts, etc.\n",
        "\n",
        "**Why Do This?** The ANN learns to map inputs to output, like 'high temperature + good substrates = more biogas'.\n",
        "\n",
        "**Tips**: Update the 'features' list to match the full-form column names (e.g., 'Carbohydrates' instead of 'S_su') after modifying your data file."
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['S_su', 'S_aa', 'S_fa', 'S_va', 'S_bu', 'S_pro', 'S_ac', 'S_IC', 'S_IN', \n",
        "            'S_I', 'Q', 'T (C)', 'X_xc', 'X_ch', 'X_pr', 'X_li', 'X_su', 'X_aa', \n",
        "            'X_fa', 'X_c4', 'X_pro', 'X_ac', 'X_h2', 'X_I', 'S_cation', 'S_anion']  # List of input column names\n",
        "\n",
        "label = 'q_gas'  # The output column name\n",
        "\n",
        "X = df[features]  # Selects input columns from the DataFrame\n",
        "y = df[label]  # Selects the output column"
      ],
      "metadata": {
        "id": "define-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Selection Using Mutual Information\n",
        "\n",
        "**Background on Feature Selection**: Feature selection is the process of identifying and selecting a subset of relevant features (inputs) for use in model construction. It helps improve model performance by reducing overfitting, shortening training times, and enhancing generalisation by removing irrelevant or redundant data. In data-driven modelling like this, with potentially many AD parameters, feature selection ensures the model focuses on the most impactful variables.\n",
        "\n",
        "![Feature Selection Schematic](https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/Feature%20Selection%20Image.png)\n",
        "\n",
        "**Description for Beginners**: Not all inputs are equally important. Mutual Information (MI) scores how much each feature 'informs' the output. A high score means the feature strongly relates to biogas production. We rank them and select the top 10 to simplify the model and focus on key variables.\n",
        "\n",
        "**Why Do This?** Too many features can confuse the model (overfitting) or slow it down. This step identifies 'VIPs' like temperature, which often impacts AD heavily.\n",
        "\n",
        "**Tips**: Change `top_n` to select more/fewer features. Run this to see which AD parameters matter most in your data."
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores = mutual_info_regression(X, y, random_state=42)  # Calculates MI scores between features and output\n",
        "mi_df = pd.DataFrame({'Feature': features, 'Mutual_Information': mi_scores})  # Creates a table with features and scores\n",
        "mi_df.sort_values(by='Mutual_Information', ascending=False, inplace=True)  # Sorts the table by score, highest first\n",
        "print(\"Feature importance based on Mutual Information:\")  # Prints a message\n",
        "display(mi_df)  # Shows the sorted table\n",
        "\n",
        "# Select top N features\n",
        "top_n = 10  # Number of top features to keep; you can change this (e.g., to 5 for simpler model)\n",
        "top_features = mi_df['Feature'].head(top_n).tolist()  # Gets the names of top features\n",
        "print(f\"Top {top_n} features selected for training: {top_features}\")  # Prints the selected features\n",
        "\n",
        "X_top = df[top_features]  # Updates X to only include top features\n",
        "\n",
        "# Feature Importance Plot (bar chart for visualisation)\n",
        "top_mi_features = mi_df.head(top_n).sort_values(by='Mutual_Information')  # Selects and sorts for horizontal bar\n",
        "plt.figure(figsize=(10,6))  # Sets plot size\n",
        "plt.barh(top_mi_features['Feature'], top_mi_features['Mutual_Information'], color='skyblue')  # Creates horizontal bar chart\n",
        "plt.xlabel('Mutual Information Score')  # Label for x-axis\n",
        "plt.title(f'Top {top_n} Features Based on Mutual Information')  # Plot title\n",
        "plt.tight_layout()  # Adjusts layout to fit labels\n",
        "plt.show()  # Displays the plot"
      ],
      "metadata": {
        "id": "feature-selection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Data Preprocessing\n",
        "\n",
        "**Description for Beginners**: Raw data varies in scale (e.g., temperature in 30-60°C, concentrations in 0-100). We 'scale' everything to 0-1 range so no feature dominates. Then, split into training (80%) for learning and testing (20%) for validation. No shuffling because AD data is time-series (order matters).\n",
        "\n",
        "**Why Do This?** ANNs learn better from normalised data. Splitting prevents 'cheating'—the model shouldn't see test data during training.\n",
        "\n",
        "**Tips**: Scaling is like converting all units to 'apples' for fair comparison. If your data has outliers, consider other scalers."
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = MinMaxScaler()  # Creates a scaler for inputs\n",
        "X_scaled = scaler_X.fit_transform(X_top)  # Fits and scales inputs to 0-1\n",
        "\n",
        "scaler_y = MinMaxScaler()  # Creates a scaler for output\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))  # Scales output (reshapes to 2D for scaler)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False  # 20% test, no shuffle for time order, random_state for reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Build the ANN Model\n",
        "\n",
        "**Description for Beginners**: An ANN is like a layered cake: input layer takes data, hidden layers process it (learning patterns), output layer gives prediction. We use 'relu' activation to handle non-linear relationships (AD processes aren't straight lines). Compile sets how the model learns (optimizer adjusts weights, loss measures error).\n",
        "\n",
        "![Simple Feedforward Neural Network](https://learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg)\n",
        "\n",
        "#### How Does an ANN Work?\n",
        "Initially, the weights of the network are randomly assigned. When input is provided to the input layer, it progresses forward, with hidden layers receiving the input combined with weights. This continues until the output layer produces a result. The result is compared to the actual value, and backpropagation adjusts the weights to improve accuracy. Neurons in the layers handle individual learning through activation functions, which determine if a signal passes based on the input from the previous layer.\n",
        "\n",
        "#### Activation Functions\n",
        "Activation functions are crucial for ANNs to learn complex, non-linear mappings between inputs and outputs. They introduce non-linearity to the network. Common types include:\n",
        "\n",
        "- **Sigmoid (Logistic)**: Outputs between 0 and 1, S-shaped curve. Easy to use but suffers from vanishing gradients, slow convergence, and non-zero-centred output.\n",
        "- **Tanh (Hyperbolic Tangent)**: Outputs between -1 and 1, zero-centred for easier optimisation, but still prone to vanishing gradients.\n",
        "- **ReLU (Rectified Linear Unit)**: Simple and efficient (max(0, x)), avoids vanishing gradients, and speeds up convergence. Widely used in hidden layers but can cause 'dead neurons'; Leaky ReLU addresses this with a small slope for negative values.\n",
        "\n",
        "For output layers, use Softmax for classification (probabilities) or linear for regression.\n",
        "\n",
        "#### What Happens Without an Activation Function?\n",
        "Without activation functions, the output would be a simple linear function, limiting the network to modelling basic relationships like linear regression. This lacks the power to handle complex, non-linear data such as images or time-series, making ANNs ineffective for advanced tasks. Activation functions enable the network to learn intricate patterns in high-dimensional data.\n",
        "\n",
        "**Why Do This?** This structure is simple yet powerful for prediction tasks like biogas output.\n",
        "\n",
        "**Tips**: Neurons (64, 32) are like brain cells; more can learn complex patterns but risk overfitting. Experiment by adding layers or changing the number of neurons!"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),              # Proper input layer\n",
        "    Dense(64, activation='relu'),                  # First hidden layer\n",
        "    Dense(32, activation='relu'),                  # Second hidden layer\n",
        "    Dense(1)                                       # Output layer (regression)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "build-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train the Model\n",
        "\n",
        "**Description for Beginners**: Training is where the ANN adjusts its internal weights to minimise errors. It 'sees' the training data multiple times (epochs). Validation uses test data to check progress. Loss should decrease over epochs.\n",
        "\n",
        "**Parameters Explained**:\n",
        "- `epochs=50`: How many times the model reviews the data.\n",
        "- `batch_size=32`: Processes data in groups of 32 for efficiency.\n",
        "- `verbose=1`: Shows progress bar.\n",
        "\n",
        "**Why Do This?** Like practising a skill—the model gets better with iterations.\n",
        "\n",
        "**Tips**: If loss stops decreasing, it's converged. Increase epochs if needed, but watch for overfitting (train loss low, val loss high)."
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()  # Starts timer\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
        "                    validation_data=(X_test, y_test), verbose=1)  # Trains the model, saves history for plots\n",
        "end_time = time.time()  # Ends timer\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")  # Prints time taken"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Make Predictions and Evaluate\n",
        "\n",
        "**Description for Beginners**: Use the trained model to predict on train/test data. Inverse scale back to original units. Metrics: R² (0-1, higher better—fit quality), RMSE (error magnitude, lower better).\n",
        "\n",
        "**Background on Overfitting and Underfitting**: Overfitting occurs when the model learns the training data too well, including noise, and performs poorly on new data. Underfitting happens when the model is too simple and doesn't capture patterns in the data. You can detect overfitting if training metrics are good but test metrics are poor; underfitting if both are poor.\n",
        "\n",
        "**Why Do This?** To quantify how accurate the model is. Good test metrics mean it generalises to new data.\n",
        "\n",
        "**Tips**: R² > 0.8 is decent for AD modelling. If low, try more data or tweak model."
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_train)  # Predicts on training data\n",
        "y_test_pred = model.predict(X_test)  # Predicts on test data\n",
        "\n",
        "# Inverse scaling to original units\n",
        "y_train_orig = scaler_y.inverse_transform(y_train)  # Unscales training output\n",
        "y_train_pred_orig = scaler_y.inverse_transform(y_train_pred)  # Unscales training predictions\n",
        "y_test_orig = scaler_y.inverse_transform(y_test)  # Unscales test output\n",
        "y_test_pred_orig = scaler_y.inverse_transform(y_test_pred)  # Unscales test predictions\n",
        "\n",
        "# Calculate metrics\n",
        "r2_train = r2_score(y_train_orig, y_train_pred_orig)  # R² for train\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_train_pred_orig))  # RMSE for train\n",
        "r2_test = r2_score(y_test_orig, y_test_pred_orig)  # R² for test\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_orig))  # RMSE for test\n",
        "\n",
        "print(f'Training R²: {r2_train:.4f} (how well it fits train data, 1=perfect), RMSE: {rmse_train:.4f} (average error)')  # Prints train metrics\n",
        "print(f'Test R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}')  # Prints test metrics"
      ],
      "metadata": {
        "id": "predict-eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Visualise Results\n",
        "\n",
        "**Description for Beginners**: Plots show actual vs. predicted biogas (lines should match closely) and loss over epochs (should decrease). This visually confirms performance.\n",
        "\n",
        "**Why Do This?** Numbers are good, but graphs reveal trends, like if predictions follow AD fluctuations.\n",
        "\n",
        "**Tips**: If lines don't overlap on test set, the model may overfit—try fewer features or more data."
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))  # 1 row, 3 columns, wide figure to fit legends\n",
        "\n",
        "# Plot 1: Training Set Actual vs Predicted\n",
        "axs[0].plot(y_train_orig, label='Actual Train', color='blue')\n",
        "axs[0].plot(y_train_pred_orig, label='Predicted Train', color='orange')\n",
        "axs[0].set_title('Training Set: Actual vs Predicted')\n",
        "axs[0].set_ylabel('q_gas')\n",
        "axs[0].set_xlabel('Time Steps')\n",
        "axs[0].legend(loc='upper left')\n",
        "\n",
        "# Plot 2: Test Set Actual vs Predicted\n",
        "axs[1].plot(y_test_orig, label='Actual Test', color='blue')\n",
        "axs[1].plot(y_test_pred_orig, label='Predicted Test', color='orange')\n",
        "axs[1].set_title('Test Set: Actual vs Predicted')\n",
        "axs[1].set_ylabel('q_gas')\n",
        "axs[1].set_xlabel('Time Steps')\n",
        "axs[1].legend(loc='upper left')\n",
        "\n",
        "# Plot 3: Model Loss During Training\n",
        "axs[2].plot(history.history['loss'], label='Train Loss', color='green')\n",
        "axs[2].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "axs[2].set_title('Model Loss During Training')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('Loss')\n",
        "axs[2].legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()  # Adjusts layout to prevent overlap\n",
        "plt.show()  # Displays all three plots in one row"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping Up\n",
        "You've completed the workshop! You've loaded data, selected key features, preprocessed it, built and trained an ANN, and evaluated its performance with metrics and plots.\n",
        "\n",
        "**Next Steps**:\n",
        "- Apply this to your real AD plant data (clean it first if needed).\n",
        "- Experiment: Change top_n, epochs, number of neurons, or add layers to improve results.\n",
        "- Explore methods to automate tuning of model parameters (hyperparameters)."
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
