{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2hP3w4v5sE3kL5zqK7p8F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rm01243/Data-Driven-Modelling-for-AD-Workshop/blob/main/AD_ANN_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data-Driven Modeling of Anaerobic Digesters Using Artificial Neural Networks (ANNs)\n",
        "\n",
        "![Anaerobic Digester Icon](https://cdn-icons-png.flaticon.com/512/12797/12797192.png)\n",
        "\n",
        "## Workshop Overview\n",
        "This notebook provides a beginner-friendly, hands-on workshop on using Artificial Neural Networks (ANNs) for modeling anaerobic digesters (AD) to predict biogas production. It is targeted at biogas researchers, AD plant operators, consultants, and sustainability professionals with little to no programming or modeling experience.\n",
        "\n",
        "### What is Anaerobic Digestion?\n",
        "Anaerobic digestion is a biological process where microorganisms break down organic material in the absence of oxygen, producing biogas (mainly methane) as a renewable energy source. Modeling this process helps predict biogas output based on inputs like temperature and substrate levels, aiding in optimization without complex physics-based equations.\n",
        "\n",
        "### What are ANNs?\n",
        "Artificial Neural Networks are computational models inspired by the human brain. They learn patterns from data to make predictions. In this workshop, we'll use a simple ANN to predict biogas flow (`q_gas`) from AD parameters.\n",
        "\n",
        "### Key Features:\n",
        "- **No Installation Required**: Runs entirely in Google Colab, a free online tool that executes Python code in your browser.\n",
        "- **Interactive**: Train a model, evaluate it, and see results with plots.\n",
        "- **Well-Documented**: Each step includes detailed explanations, code comments, and tips for beginners.\n",
        "- **Data**: Uses synthetic AD data (`AD_Synthetic_Data.xlsx`) loaded directly from the GitHub repository. Synthetic data is artificially generated to mimic real AD processes for demonstration.\n",
        "- **Topics Covered**: Data loading, feature selection, preprocessing, ANN building/training, evaluation, and visualization.\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic understanding of anaerobic digestion (AD) processes (e.g., what substrates and temperature affect biogas).\n",
        "- Access to Google Colab (free, browser-based—no software needed).\n",
        "- No coding background required—we'll explain every line!\n",
        "\n",
        "### How to Use This Notebook:\n",
        "1. Open in Colab (click the badge above).\n",
        "2. Run cells sequentially (Shift+Enter or click the play button). Markdown cells (like this) explain concepts; code cells execute Python code.\n",
        "3. Experiment by modifying parameters (e.g., number of epochs in training, or top features in selection) and re-run cells to see changes.\n",
        "4. If something goes wrong, restart the runtime (Runtime > Restart runtime) and re-run from the top.\n",
        "\n",
        "If you encounter issues, check the output logs or restart the runtime. For real-world use, validate with actual plant data. Let's start modeling!"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "**Description for Beginners**: Libraries are pre-written code packages that provide tools for tasks like data handling or modeling. We 'import' them to use their functions. For example, Pandas helps read Excel files, and Keras builds the ANN.\n",
        "\n",
        "**Libraries Explained**:\n",
        "- `numpy` (np): For numerical operations, like arrays.\n",
        "- `pandas` (pd): For loading and manipulating data tables.\n",
        "- `matplotlib.pyplot` (plt): For creating plots and visualizations.\n",
        "- `time`: To measure how long code takes to run.\n",
        "- `sklearn` modules: For splitting data, scaling, metrics, and feature selection.\n",
        "- `tensorflow.keras`: For building and training the ANN (TensorFlow is the backend, Keras is the user-friendly interface).\n",
        "\n",
        "**Why Do This?** These tools save time— no need to write everything from scratch.\n",
        "\n",
        "**Note**: Colab has these pre-installed, so just run the cell."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # For numerical computations and arrays\n",
        "import pandas as pd  # For reading and handling data in table format\n",
        "import matplotlib.pyplot as plt  # For creating plots and graphs\n",
        "import time  # To time how long the code takes\n",
        "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
        "from sklearn.preprocessing import MinMaxScaler  # To scale data to a 0-1 range\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # To evaluate model performance\n",
        "from sklearn.feature_selection import mutual_info_regression  # For selecting important features\n",
        "from tensorflow.keras.models import Sequential  # To build the ANN layer by layer\n",
        "from tensorflow.keras.layers import Dense  # To add layers to the ANN"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "**Description for Beginners**: We read the Excel file from the GitHub repository into a 'DataFrame' (df), which is like an Excel table in Python. This lets us access columns easily. We'll display the first 5 rows to check the data.\n",
        "\n",
        "**Why Do This?** To ensure the data is loaded correctly and understand its structure (e.g., column names like 'S_su' for sugar substrate).\n",
        "\n",
        "**Tips**: If the sheet name is different, change 'Sheet1'. For your own data, you can replace the URL with your file's raw GitHub URL."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/AD_Synthetic_Data.xlsx'  # URL to the data file in the repo\n",
        "df = pd.read_excel(data_url, sheet_name='Sheet1')  # Reads the Excel file from URL into a DataFrame\n",
        "print(\"Data preview (first 5 rows):\")  # Prints a message\n",
        "display(df.head())  # Shows the first 5 rows in a nice table format"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Inputs and Output\n",
        "\n",
        "**Description for Beginners**: Inputs (features) are the variables that influence biogas production, like temperature or substrate levels. The output (label) is what we want to predict: 'q_gas' (biogas flow rate). We separate them into X (inputs) and y (output).\n",
        "\n",
        "**Features Explained** (based on typical AD models like ADM1):\n",
        "- `S_*`: Soluble substrates (e.g., `S_su` = sugars, `S_aa` = amino acids)—these are broken down by microbes.\n",
        "- `X_*`: Particulate biomass (e.g., `X_su` = sugar degraders).\n",
        "- `Q`: Inflow rate, `T (C)`: Temperature in Celsius—key for microbial activity.\n",
        "- Others: Ions, inerts, etc.\n",
        "\n",
        "**Why Do This?** The ANN learns to map inputs to output, like 'high temperature + good substrates = more biogas'.\n",
        "\n",
        "**Tips**: If your data has different columns, update the 'features' list."
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['S_su', 'S_aa', 'S_fa', 'S_va', 'S_bu', 'S_pro', 'S_ac', 'S_IC', 'S_IN', \n",
        "            'S_I', 'Q', 'T (C)', 'X_xc', 'X_ch', 'X_pr', 'X_li', 'X_su', 'X_aa', \n",
        "            'X_fa', 'X_c4', 'X_pro', 'X_ac', 'X_h2', 'X_I', 'S_cation', 'S_anion']  # List of input column names\n",
        "\n",
        "label = 'q_gas'  # The output column name\n",
        "\n",
        "X = df[features]  # Selects input columns from the DataFrame\n",
        "y = df[label]  # Selects the output column"
      ],
      "metadata": {
        "id": "define-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Selection Using Mutual Information\n",
        "\n",
        "**Description for Beginners**: Not all inputs are equally important. Mutual Information (MI) scores how much each feature 'informs' the output. A high score means the feature strongly relates to biogas production. We rank them and select the top 10 to simplify the model and focus on key variables.\n",
        "\n",
        "![Mutual Information Feature Selection Example](https://scikit-learn.org/stable/_images/sphx_glr_plot_f_test_vs_mi_001.png)\n",
        "\n",
        "**Why Do This?** Too many features can confuse the model (overfitting) or slow it down. This step identifies 'VIPs' like temperature, which often impacts AD heavily.\n",
        "\n",
        "**Tips**: Change `top_n` to select more/fewer features. Run this to see which AD parameters matter most in your data."
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores = mutual_info_regression(X, y, random_state=42)  # Calculates MI scores between features and output\n",
        "mi_df = pd.DataFrame({'Feature': features, 'Mutual_Information': mi_scores})  # Creates a table with features and scores\n",
        "mi_df.sort_values(by='Mutual_Information', ascending=False, inplace=True)  # Sorts the table by score, highest first\n",
        "print(\"Feature importance based on Mutual Information:\")  # Prints a message\n",
        "display(mi_df)  # Shows the sorted table\n",
        "\n",
        "# Select top N features\n",
        "top_n = 10  # Number of top features to keep; you can change this (e.g., to 5 for simpler model)\n",
        "top_features = mi_df['Feature'].head(top_n).tolist()  # Gets the names of top features\n",
        "print(f\"Top {top_n} features selected for training: {top_features}\")  # Prints the selected features\n",
        "\n",
        "X_top = df[top_features]  # Updates X to only include top features\n",
        "\n",
        "# Feature Importance Plot (bar chart for visualization)\n",
        "top_mi_features = mi_df.head(top_n).sort_values(by='Mutual_Information')  # Selects and sorts for horizontal bar\n",
        "plt.figure(figsize=(10,6))  # Sets plot size\n",
        "plt.barh(top_mi_features['Feature'], top_mi_features['Mutual_Information'], color='skyblue')  # Creates horizontal bar chart\n",
        "plt.xlabel('Mutual Information Score')  # Label for x-axis\n",
        "plt.title(f'Top {top_n} Features Based on Mutual Information')  # Plot title\n",
        "plt.tight_layout()  # Adjusts layout to fit labels\n",
        "plt.show()  # Displays the plot"
      ],
      "metadata": {
        "id": "feature-selection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Data Preprocessing\n",
        "\n",
        "**Description for Beginners**: Raw data varies in scale (e.g., temperature in 30-60°C, concentrations in 0-100). We 'scale' everything to 0-1 range so no feature dominates. Then, split into training (80%) for learning and testing (20%) for validation. No shuffling because AD data is time-series (order matters).\n",
        "\n",
        "![Min-Max Scaling Diagram](https://miro.medium.com/v2/resize:fit:1400/1*CWenX2OYfFFvCLXwwF8aGw.png)\n",
        "\n",
        "**Why Do This?** ANNs learn better from normalized data. Splitting prevents 'cheating'—the model shouldn't see test data during training.\n",
        "\n",
        "**Tips**: Scaling is like converting all units to 'apples' for fair comparison. If your data has outliers, consider other scalers."
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = MinMaxScaler()  # Creates a scaler for inputs\n",
        "X_scaled = scaler_X.fit_transform(X_top)  # Fits and scales inputs to 0-1\n",
        "\n",
        "scaler_y = MinMaxScaler()  # Creates a scaler for output\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))  # Scales output (reshapes to 2D for scaler)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False  # 20% test, no shuffle for time order, random_state for reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Build the ANN Model\n",
        "\n",
        "**Description for Beginners**: An ANN is like a layered cake: input layer takes data, hidden layers process it (learning patterns), output layer gives prediction. We use 'relu' activation to handle non-linear relationships (AD processes aren't straight lines). Compile sets how the model learns (optimizer adjusts weights, loss measures error).\n",
        "\n",
        "![Simple Feedforward Neural Network](https://learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg)\n",
        "\n",
        "**Why Do This?** This structure is simple yet powerful for prediction tasks like biogas output.\n",
        "\n",
        "**Tips**: Neurons (64, 32) are like brain cells; more can learn complex patterns but risk overfitting. Experiment by adding layers!"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()  # Starts a new ANN model\n",
        "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))  # Adds first hidden layer with 64 neurons, relu for non-linearity\n",
        "model.add(Dense(32, activation='relu'))  # Adds second hidden layer with 32 neurons\n",
        "model.add(Dense(1))  # Adds output layer with 1 neuron (for single prediction)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')  # Sets optimizer (adam: efficient learner) and loss (MSE: for regression errors)"
      ],
      "metadata": {
        "id": "build-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train the Model\n",
        "\n",
        "**Description for Beginners**: Training is where the ANN adjusts its internal weights to minimize errors. It 'sees' the training data multiple times (epochs). Validation uses test data to check progress. Loss should decrease over epochs.\n",
        "\n",
        "**Parameters Explained**:\n",
        "- `epochs=20`: How many times the model reviews the data.\n",
        "- `batch_size=32`: Processes data in groups of 32 for efficiency.\n",
        "- `verbose=1`: Shows progress bar.\n",
        "\n",
        "**Why Do This?** Like practicing a skill—the model gets better with iterations.\n",
        "\n",
        "**Tips**: If loss stops decreasing, it's converged. Increase epochs if needed, but watch for overfitting (train loss low, val loss high)."
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()  # Starts timer\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, \n",
        "                    validation_data=(X_test, y_test), verbose=1)  # Trains the model, saves history for plots\n",
        "end_time = time.time()  # Ends timer\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")  # Prints time taken"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Make Predictions and Evaluate\n",
        "\n",
        "**Description for Beginners**: Use the trained model to predict on train/test data. Inverse scale back to original units. Metrics: R² (0-1, higher better—fit quality), RMSE (error magnitude, lower better).\n",
        "\n",
        "**Why Do This?** To quantify how accurate the model is. Good test metrics mean it generalizes to new data.\n",
        "\n",
        "**Tips**: R² > 0.8 is decent for AD modeling. If low, try more data or tweak model."
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_train)  # Predicts on training data\n",
        "y_test_pred = model.predict(X_test)  # Predicts on test data\n",
        "\n",
        "# Inverse scaling to original units\n",
        "y_train_orig = scaler_y.inverse_transform(y_train)  # Unscales training output\n",
        "y_train_pred_orig = scaler_y.inverse_transform(y_train_pred)  # Unscales training predictions\n",
        "y_test_orig = scaler_y.inverse_transform(y_test)  # Unscales test output\n",
        "y_test_pred_orig = scaler_y.inverse_transform(y_test_pred)  # Unscales test predictions\n",
        "\n",
        "# Calculate metrics\n",
        "r2_train = r2_score(y_train_orig, y_train_pred_orig)  # R² for train\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_train_pred_orig))  # RMSE for train\n",
        "r2_test = r2_score(y_test_orig, y_test_pred_orig)  # R² for test\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_orig))  # RMSE for test\n",
        "\n",
        "print(f'Training R²: {r2_train:.4f} (how well it fits train data, 1=perfect), RMSE: {rmse_train:.4f} (average error)')  # Prints train metrics\n",
        "print(f'Test R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}')  # Prints test metrics"
      ],
      "metadata": {
        "id": "predict-eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Visualize Results\n",
        "\n",
        "**Description for Beginners**: Plots show actual vs. predicted biogas (lines should match closely) and loss over epochs (should decrease). This visually confirms performance.\n",
        "\n",
        "**Why Do This?** Numbers are good, but graphs reveal trends, like if predictions follow AD fluctuations.\n",
        "\n",
        "**Tips**: If lines don't overlap on test set, the model may overfit—try fewer features or more data."
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Plotly if not already installed (run this once)\n",
        "!pip install plotly\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Create subplots with 1 row and 3 columns\n",
        "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
        "    'Training Set: Actual vs Predicted Biogas Production',\n",
        "    'Test Set: Actual vs Predicted Biogas Production',\n",
        "    'Model Loss During Training'\n",
        "))\n",
        "\n",
        "# Plot 1: Training Set\n",
        "fig.add_trace(go.Scatter(y=y_train_orig.flatten(), mode='lines', name='Actual Train', line=dict(color='blue')), row=1, col=1)\n",
        "fig.add_trace(go.Scatter(y=y_train_pred_orig.flatten(), mode='lines', name='Predicted Train', line=dict(color='orange')), row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Time Steps\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"q_gas\", row=1, col=1)\n",
        "\n",
        "# Plot 2: Test Set\n",
        "fig.add_trace(go.Scatter(y=y_test_orig.flatten(), mode='lines', name='Actual Test', line=dict(color='blue')), row=1, col=2)\n",
        "fig.add_trace(go.Scatter(y=y_test_pred_orig.flatten(), mode='lines', name='Predicted Test', line=dict(color='orange')), row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Time Steps\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"q_gas\", row=1, col=2)\n",
        "\n",
        "# Plot 3: Loss\n",
        "fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train Loss', line=dict(color='green')), row=1, col=3)\n",
        "fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation Loss', line=dict(color='red')), row=1, col=3)\n",
        "fig.update_xaxes(title_text=\"Epoch\", row=1, col=3)\n",
        "fig.update_yaxes(title_text=\"Loss\", row=1, col=3)\n",
        "\n",
        "# Update layout for better appearance\n",
        "fig.update_layout(height=500, width=1500, showlegend=True, template='plotly_white')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping Up\n",
        "You've completed the workshop! You've loaded data, selected key features, preprocessed it, built and trained an ANN, and evaluated its performance with metrics and plots.\n",
        "\n",
        "**Next Steps**:\n",
        "- Apply this to your real AD plant data (clean it first if needed).\n",
        "- Experiment: Change top_n, epochs, or add layers to improve results.\n",
        "- For advanced: Explore LSTM networks for better time-series prediction."
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
