{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2hP3w4v5sE3kL5zqK7p8F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rm01243/Data-Driven-Modelling-for-AD-Workshop/blob/main/AD_ANN_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data-Driven Modelling of Anaerobic Digesters Using Artificial Neural Networks (ANNs)\n",
        "\n",
        "![Data Driven Modelling Image](https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/Data%20Driven%20Modelling%20Image.png)\n",
        "\n",
        "## Workshop Overview\n",
        "This notebook provides a beginner-friendly, hands-on workshop on using Artificial Neural Networks (ANNs) for modelling anaerobic digesters (AD) to predict biogas production. It is targeted at biogas researchers, AD plant operators, consultants, and sustainability professionals with little to no programming or modelling experience.\n",
        "\n",
        "### What is Anaerobic Digestion?\n",
        "Anaerobic digestion is a biological process where microorganisms break down organic material in the absence of oxygen, producing biogas (mainly methane) as a renewable energy source. Modelling this process helps predict biogas output based on inputs like temperature and substrate levels, aiding in optimisation without complex physics-based equations.\n",
        "\n",
        "### What are ANNs?\n",
        "Artificial Neural Networks are computational models inspired by the human brain. They learn patterns from data to make predictions. In this workshop, we'll use a simple ANN to predict biogas flow (`q_gas`) from AD parameters.\n",
        "\n",
        "### Key Features:\n",
        "- **No Installation Required**: Runs entirely in Google Colab, a free online tool that executes Python code in your browser.\n",
        "- **Interactive**: Train a model, evaluate it, and see results with plots.\n",
        "- **Data**: Uses synthetic AD data (`AD_Synthetic_Data.xlsx`) loaded directly from the GitHub repository. Synthetic data is artificially generated to mimic real AD processes for demonstration.\n",
        "- **Topics Covered**: Data loading, feature selection, preprocessing, ANN building/training, evaluation, and visualisation.\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic understanding of anaerobic digestion (AD) processes (e.g., what substrates and temperature affect biogas).\n",
        "- Access to Google Colab (free, browser-based—no software needed).\n",
        "- No coding background required—we'll explain every line!\n",
        "\n",
        "### How to Use This Notebook:\n",
        "1. Open in Colab (click the badge above).\n",
        "2. Run cells sequentially (Shift+Enter or click the play button). Markdown cells (like this) explain concepts; code cells execute Python code.\n",
        "3. Experiment by modifying parameters (e.g., number of epochs in training, or top features in selection) and re-run cells to see changes.\n",
        "4. If something goes wrong, restart the runtime (Runtime > Restart runtime) and re-run from the top."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "**Description for Beginners**: Libraries are pre-written code packages that provide tools for tasks like data handling or modelling. We 'import' them to use their functions. For example, Pandas helps read Excel files, and Keras builds the ANN.\n",
        "\n",
        "**Libraries Explained**:\n",
        "- `numpy` (np): For numerical operations, like arrays.\n",
        "- `pandas` (pd): For loading and manipulating data tables.\n",
        "- `matplotlib.pyplot` (plt): For creating plots and visualisations.\n",
        "- `time`: To measure how long code takes to run.\n",
        "- `sklearn` modules: For splitting data, scaling, metrics, and feature selection.\n",
        "- `tensorflow.keras`: For building and training the ANN (TensorFlow is the backend, Keras is the user-friendly interface).\n",
        "\n",
        "**Why Do This?** These tools save time—no need to write everything from scratch.\n",
        "\n",
        "**Note**: Colab has these pre-installed, so just run the cell."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # For numerical computations and arrays\n",
        "import pandas as pd  # For reading and handling data in table format\n",
        "import matplotlib.pyplot as plt  # For creating plots and graphs\n",
        "import time  # To time how long the code takes\n",
        "import tensorflow as tf  # For setting random seed\n",
        "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
        "from sklearn.preprocessing import MinMaxScaler  # To scale data to a 0-1 range\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # To evaluate model performance\n",
        "from sklearn.feature_selection import mutual_info_regression  # For selecting important features\n",
        "from tensorflow.keras.models import Sequential  # To build the ANN layer by layer\n",
        "from tensorflow.keras.layers import Dense  # To add layers to the ANN\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "**Description for Beginners**: We read the Excel file from the GitHub repository into a 'DataFrame' (df), which is like an Excel table in Python. This lets us access columns easily. We'll display the first 5 rows to check the data.\n",
        "\n",
        "**Why Do This?** To ensure the data is loaded correctly and understand its structure.\n",
        "\n",
        "**Tips**: If the sheet name is different, change 'Sheet1'. For your own data, you can replace the URL with your file's raw GitHub URL. Update column names in the data file to full forms like 'Carbohydrates' for clarity, and adjust the 'features' list in Step 3 accordingly."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/AD_Synthetic_Data.xlsx'  # URL to the data file in the repo\n",
        "df = pd.read_excel(data_url, sheet_name='Sheet1')  # Reads the Excel file from URL into a DataFrame\n",
        "print(\"Data preview (first 5 rows):\")  # Prints a message\n",
        "display(df.head())  # Shows the first 5 rows in a nice table format"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Inputs and Output\n",
        "\n",
        "**Description for Beginners**: Inputs (features) are the variables that influence biogas production, like temperature or substrate levels. The output (label) is what we want to predict: 'q_gas' (biogas flow rate). We separate them into X (inputs) and y (output).\n",
        "\n",
        "**Features Explained** (based on typical AD models like ADM1):\n",
        "- `S_*`: Soluble substrates (e.g., `S_su` = sugars, `S_aa` = amino acids)—these are broken down by microbes.\n",
        "- `X_*`: Particulate biomass (e.g., `X_su` = sugar degraders).\n",
        "- `Q`: Inflow rate, `T (C)`: Temperature in Celsius—key for microbial activity.\n",
        "- Others: Ions, inerts, etc.\n",
        "\n",
        "**Why Do This?** The ANN learns to map inputs to output, like 'high temperature + good substrates = more biogas'.\n",
        "\n",
        "**Tips**: Update the 'features' list to match the full-form column names (e.g., 'Carbohydrates' instead of 'S_su') after modifying your data file."
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['S_su', 'S_aa', 'S_fa', 'S_va', 'S_bu', 'S_pro', 'S_ac', 'S_IC', 'S_IN', \n",
        "            'S_I', 'Q', 'T (C)', 'X_xc', 'X_ch', 'X_pr', 'X_li', 'X_su', 'X_aa', \n",
        "            'X_fa', 'X_c4', 'X_pro', 'X_ac', 'X_h2', 'X_I', 'S_cation', 'S_anion']  # List of input column names\n",
        "\n",
        "label = 'q_gas'  # The output column name\n",
        "\n",
        "X = df[features]  # Selects input columns from the DataFrame\n",
        "y = df[label]  # Selects the output column"
      ],
      "metadata": {
        "id": "define-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Selection Using Mutual Information\n",
        "\n",
        "**Background on Feature Selection**: Feature selection is the process of identifying and selecting a subset of relevant features (inputs) for use in model construction. It helps improve model performance by reducing overfitting, shortening training times, and enhancing generalisation by removing irrelevant or redundant data. In data-driven modelling like this, with potentially many AD parameters, feature selection ensures the model focuses on the most impactful variables.\n",
        "\n",
        "![Feature Selection Schematic](https://raw.githubusercontent.com/rm01243/Data-Driven-Modelling-for-AD-Workshop/main/Feature%20Selection%20Image.png)\n",
        "\n",
        "**Description for Beginners**: Not all inputs are equally important. Mutual Information (MI) scores how much each feature 'informs' the output. A high score means the feature strongly relates to biogas production. We rank them and select the top 10 to simplify the model and focus on key variables.\n",
        "\n",
        "**Why Do This?** Too many features can confuse the model (overfitting) or slow it down. This step identifies 'VIPs' like temperature, which often impacts AD heavily.\n",
        "\n",
        "**Tips**: Change `top_n` to select more/fewer features. Run this to see which AD parameters matter most in your data."
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores = mutual_info_regression(X, y, random_state=42)  # Calculates MI scores between features and output\n",
        "mi_df = pd.DataFrame({'Feature': features, 'Mutual_Information': mi_scores})  # Creates a table with features and scores\n",
        "mi_df.sort_values(by='Mutual_Information', ascending=False, inplace=True)  # Sorts the table by score, highest first\n",
        "print(\"Feature importance based on Mutual Information:\")  # Prints a message\n",
        "display(mi_df)  # Shows the sorted table\n",
        "\n",
        "# Select top N features\n",
        "top_n = 10  # Number of top features to keep; you can change this (e.g., to 5 for simpler model)\n",
        "top_features = mi_df['Feature'].head(top_n).tolist()  # Gets the names of top features\n",
        "print(f\"Top {top_n} features selected for training: {top_features}\")  # Prints the selected features\n",
        "\n",
        "X_top = df[top_features]  # Updates X to only include top features\n",
        "\n",
        "# Feature Importance Plot (bar chart for visualisation)\n",
        "top_mi_features = mi_df.head(top_n).sort_values(by='Mutual_Information')  # Selects and sorts for horizontal bar\n",
        "plt.figure(figsize=(10,6))  # Sets plot size\n",
        "plt.barh(top_mi_features['Feature'], top_mi_features['Mutual_Information'], color='skyblue')  # Creates horizontal bar chart\n",
        "plt.xlabel('Mutual Information Score')  # Label for x-axis\n",
        "plt.title(f'Top {top_n} Features Based on Mutual Information')  # Plot title\n",
        "plt.tight_layout()  # Adjusts layout to fit labels\n",
        "plt.show()  # Displays the plot"
      ],
      "metadata": {
        "id": "feature-selection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Data Preprocessing\n",
        "\n",
        "**Description for Beginners**: Raw data varies in scale (e.g., temperature in 30-60°C, concentrations in 0-100). We 'scale' everything to 0-1 range so no feature dominates. Then, split into training (80%) for learning and testing (20%) for validation. No shuffling because AD data is time-series (order matters).\n",
        "\n",
        "**Why Do This?** ANNs learn better from normalised data. Splitting prevents 'cheating'—the model shouldn't see test data during training.\n",
        "\n",
        "**Tips**: Scaling is like converting all units to 'apples' for fair comparison. If your data has outliers, consider other scalers."
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = MinMaxScaler()  # Creates a scaler for inputs\n",
        "X_scaled = scaler_X.fit_transform(X_top)  # Fits and scales inputs to 0-1\n",
        "\n",
        "scaler_y = MinMaxScaler()  # Creates a scaler for output\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))  # Scales output (reshapes to 2D for scaler)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False  # 20% test, no shuffle for time order, random_state for reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Neural Network\n",
        "There are multiple types of neural network, each of which come with their own specific use cases and levels of complexity. The most basic type of neural net is something called a feedforward neural network, in which information travels in only one direction from input to output. A more widely used type of network is the recurrent neural network, in which data can flow in multiple directions. These neural networks possess greater learning abilities and are widely employed for more complex tasks such as learning handwriting or language recognition."
      ],
      "metadata": {
        "id": "types-nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Does ANN Work?\n",
        "Initially the weights of the network can be randomly assigned. When the input is given to the input layer, the process moves forward and the hidden layer receives the input combined with the weights. This process continues until the final layer of output is reached and the result is given. When the result is out, it is compared to the actual value, and a back propagation algorithm comes into play to adjust the weights of the network linkages to better the result. What do the neurons in the layers then do? They are responsible for the learning individually. They consist of activation functions that allow the signal to pass or not depending on which activation function is being used and what input came from the previous layer."
      ],
      "metadata": {
        "id": "how-ann-works"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Function\n",
        "Activation functions are really important for an Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable. They introduce non-linear properties to our Network. Their main purpose is to convert an input signal of a node in an A-NN to an output signal. That output signal now is used as an input in the next layer in the stack.\n",
        "Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply an Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n",
        "Most popular types of Activation functions -\n",
        "Sigmoid or Logistic\n",
        "Tanh — Hyperbolic tangent\n",
        "ReLu -Rectified linear units\n",
        "Sigmoid Activation function: It is an activation function of form f(x) = 1 / 1 + exp(-x) . Its Range is between 0 and 1. It is a S — shaped curve. It is easy to understand and apply but it has major reasons which have made it fall out of popularity -\n",
        "\n",
        "Vanishing gradient problem\n",
        "Secondly , its output isn’t zero centred. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimisation harder.\n",
        "Sigmoids saturate and kill gradients.\n",
        "Sigmoids have slow convergence.\n",
        " Hyperbolic Tangent function- Tanh : It’s mathematical formula is f(x) = 1 — exp(-2x) / 1 + exp(-2x). Now it’s output is zero centred because its range in between -1 to 1 i.e -1 < output < 1 . Hence optimisation is easier in this method hence in practice it is always preferred over Sigmoid function . But still it suffers from Vanishing gradient problem. ReLu- Rectified Linear units : It has become very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function. It’s just R(x) = max(0,x) i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x. Hence as seeing the mathematical form of this function we can see that it is very simple and efficient . A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. Hence hence it avoids and rectifies vanishing gradient problem . Almost all deep learning Models use ReLu nowadays.\n",
        "\n",
        "But its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n",
        "\n",
        "Hence for output layers we should use a Softmax function for a Classification problem to compute the probabilities for the classes , and for a regression problem it should simply use a linear function.\n",
        "\n",
        "Another problem with ReLu is that some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n",
        "\n",
        "To fix this problem another modification was introduced called Leaky ReLu to fix the problem of dying neurons. It introduces a small slope to keep the updates alive."
      ],
      "metadata": {
        "id": "activation-function"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Happens Without Activation Function?\n",
        "If we do not apply an Activation function then the output signal would simply be a simple linear function. A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not perform good most of the times. We want our Neural Network to not just learn and compute a linear function but something more complicated than that. Also without activation function our Neural network would not be able to learn and model other complicated kinds of data such as images, videos , audio , speech etc. That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear -big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets."
      ],
      "metadata": {
        "id": "without-activation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Build the ANN Model\n",
        "\n",
        "**Description for Beginners**: An ANN is like a layered cake: input layer takes data, hidden layers process it (learning patterns), output layer gives prediction. We use 'relu' activation to handle non-linear relationships (AD processes aren't straight lines). Compile sets how the model learns (optimizer adjusts weights, loss measures error).\n",
        "\n",
        "![Simple Feedforward Neural Network](https://learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg)\n",
        "\n",
        "**Why Do This?** This structure is simple yet powerful for prediction tasks like biogas output.\n",
        "\n",
        "**Tips**: Neurons (64, 32) are like brain cells; more can learn complex patterns but risk overfitting. Experiment by adding layers or changing the number of neurons!"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),              # Proper input layer\n",
        "    Dense(64, activation='relu'),                  # First hidden layer\n",
        "    Dense(32, activation='relu'),                  # Second hidden layer\n",
        "    Dense(1)                                       # Output layer (regression)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "build-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train the Model\n",
        "\n",
        "**Description for Beginners**: Training is where the ANN adjusts its internal weights to minimise errors. It 'sees' the training data multiple times (epochs). Validation uses test data to check progress. Loss should decrease over epochs.\n",
        "\n",
        "**Parameters Explained**:\n",
        "- `epochs=50`: How many times the model reviews the data.\n",
        "- `batch_size=32`: Processes data in groups of 32 for efficiency.\n",
        "- `verbose=1`: Shows progress bar.\n",
        "\n",
        "**Why Do This?** Like practising a skill—the model gets better with iterations.\n",
        "\n",
        "**Tips**: If loss stops decreasing, it's converged. Increase epochs if needed, but watch for overfitting (train loss low, val loss high)."
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()  # Starts timer\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
        "                    validation_data=(X_test, y_test), verbose=1)  # Trains the model, saves history for plots\n",
        "end_time = time.time()  # Ends timer\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")  # Prints time taken"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Make Predictions and Evaluate\n",
        "\n",
        "**Description for Beginners**: Use the trained model to predict on train/test data. Inverse scale back to original units. Metrics: R² (0-1, higher better—fit quality), RMSE (error magnitude, lower better).\n",
        "\n",
        "**Background on Overfitting and Underfitting**: Overfitting occurs when the model learns the training data too well, including noise, and performs poorly on new data. Underfitting happens when the model is too simple and doesn't capture patterns in the data. You can detect overfitting if training metrics are good but test metrics are poor; underfitting if both are poor.\n",
        "\n",
        "**Why Do This?** To quantify how accurate the model is. Good test metrics mean it generalises to new data.\n",
        "\n",
        "**Tips**: R² > 0.8 is decent for AD modelling. If low, try more data or tweak model."
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_train)  # Predicts on training data\n",
        "y_test_pred = model.predict(X_test)  # Predicts on test data\n",
        "\n",
        "# Inverse scaling to original units\n",
        "y_train_orig = scaler_y.inverse_transform(y_train)  # Unscales training output\n",
        "y_train_pred_orig = scaler_y.inverse_transform(y_train_pred)  # Unscales training predictions\n",
        "y_test_orig = scaler_y.inverse_transform(y_test)  # Unscales test output\n",
        "y_test_pred_orig = scaler_y.inverse_transform(y_test_pred)  # Unscales test predictions\n",
        "\n",
        "# Calculate metrics\n",
        "r2_train = r2_score(y_train_orig, y_train_pred_orig)  # R² for train\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_train_pred_orig))  # RMSE for train\n",
        "r2_test = r2_score(y_test_orig, y_test_pred_orig)  # R² for test\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_orig))  # RMSE for test\n",
        "\n",
        "print(f'Training R²: {r2_train:.4f} (how well it fits train data, 1=perfect), RMSE: {rmse_train:.4f} (average error)')  # Prints train metrics\n",
        "print(f'Test R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}')  # Prints test metrics"
      ],
      "metadata": {
        "id": "predict-eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Visualise Results\n",
        "\n",
        "**Description for Beginners**: Plots show actual vs. predicted biogas (lines should match closely) and loss over epochs (should decrease). This visually confirms performance.\n",
        "\n",
        "**Why Do This?** Numbers are good, but graphs reveal trends, like if predictions follow AD fluctuations.\n",
        "\n",
        "**Tips**: If lines don't overlap on test set, the model may overfit—try fewer features or more data."
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))  # 1 row, 3 columns, wide figure to fit legends\n",
        "\n",
        "# Plot 1: Training Set Actual vs Predicted\n",
        "axs[0].plot(y_train_orig, label='Actual Train', color='blue')\n",
        "axs[0].plot(y_train_pred_orig, label='Predicted Train', color='orange')\n",
        "axs[0].set_title('Training Set: Actual vs Predicted')\n",
        "axs[0].set_ylabel('q_gas')\n",
        "axs[0].set_xlabel('Time Steps')\n",
        "axs[0].legend(loc='upper left')\n",
        "\n",
        "# Plot 2: Test Set Actual vs Predicted\n",
        "axs[1].plot(y_test_orig, label='Actual Test', color='blue')\n",
        "axs[1].plot(y_test_pred_orig, label='Predicted Test', color='orange')\n",
        "axs[1].set_title('Test Set: Actual vs Predicted')\n",
        "axs[1].set_ylabel('q_gas')\n",
        "axs[1].set_xlabel('Time Steps')\n",
        "axs[1].legend(loc='upper left')\n",
        "\n",
        "# Plot 3: Model Loss During Training\n",
        "axs[2].plot(history.history['loss'], label='Train Loss', color='green')\n",
        "axs[2].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "axs[2].set_title('Model Loss During Training')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('Loss')\n",
        "axs[2].legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()  # Adjusts layout to prevent overlap\n",
        "plt.show()  # Displays all three plots in one row"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping Up\n",
        "You've completed the workshop! You've loaded data, selected key features, preprocessed it, built and trained an ANN, and evaluated its performance with metrics and plots.\n",
        "\n",
        "**Next Steps**:\n",
        "- Apply this to your real AD plant data (clean it first if needed).\n",
        "- Experiment: Change top_n, epochs, number of neurons, or add layers to improve results.\n",
        "- Explore methods to automate tuning of model parameters (hyperparameters)."
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
